{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==============================================================================\n",
    "### SCRIPT NAME: 01_CS_Check_Structure_Clean V3\n",
    "### PURPOSE: Data structuring & Data cleaning\n",
    "### PACKAGES NEEDED: os, numpy, pandas, matplotlib, seaborn\n",
    "### =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will be introduced to data structuring and data cleaning in Python. Many of the introduced concepts will be  used throughout the rest of the course.\n",
    "\n",
    "This session covers typical day 1 checks when receiving new data:\n",
    "1. Descriptives\n",
    "2. Data points over time\n",
    "3. Missing values\n",
    "4. Values of categorical variables\n",
    "\n",
    "Whenever in doubt you can always look at some of these resources:\n",
    "\n",
    "    help([function name]): Provides a detailed description of the function/\n",
    "    online pandas documentation: https://pandas.pydata.org/pandas-docs/stable/?v=20200107131408\n",
    "    online matplotlib documentation: https://matplotlib.org/contents.html?v=20200131112331\n",
    "    online seaborn documentation: https://seaborn.pydata.org/\n",
    "\n",
    "Note that whenever you see multiple consecutive question marks (like '???') you will have to enter something. Evaluating a cell can be done by clicking on the 'run' button at the top or by pressing shift + enter on a selected cell.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Links to the Exercises\n",
    "\n",
    "[Exercise 1 - Data validation / completeness check](#exercise1)  \n",
    "[Exercise 2 - Data Aggregation](#exercise2)  \n",
    "[Exercise 3 - Perform format corrections](#exercise3)  \n",
    "[Exercise 4 - Join and Merge datasets](#exercise4)  \n",
    "[Exercise 5 - Time-shifting data](#exercise5)  \n",
    "[Exercise 6 - Equalizing Time Behavior](#exercise6)  \n",
    "[Exercise 7 - Calculating synthetic variables](#exercise7)  \n",
    "[Exercise 8 - Handling outliers](#exercise8)  \n",
    "[Exercise 9 - Handling missing values](#exercise9)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required packages #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages\n",
    "    import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are commonly used for Data Anlytics.  They extend Python with new functions.\n",
    "import os                             # Interactions with OS\n",
    "import numpy as np                    # Numerical python - for math/algebra/matrix/arrays etc.\n",
    "import pandas as pd                   # for manipulating flat files with for analytics (uses numpy)\n",
    "import matplotlib.pyplot as plt       # Basic chart plotting\n",
    "import seaborn as sns                 # More advanced and easier-to-use chart plotting (uses matplotlib)\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter/python notebook basics?\n",
    "    What gets printed\n",
    "    What is a variable, how does the = actually work?\n",
    "    List of shortcuts for jupyter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are cells in jupyter notebook?\n",
    "Cells can be either Code (Python) or \"Markdown. Markdown is a very simple coding language to **format text**\n",
    "\n",
    "For a selected cell (blue line to the left), you can switch between these in the menu above: Cell/Cell Type\n",
    "\n",
    "Python cells have a \"**In `[ ]`:**\" to the left, Markdown cells don't.\n",
    "\n",
    "### You can add/delete/move (and more) cells in the menu above (Edit, Insert), but there are also shortcuts\n",
    "\n",
    "- ESC + A: Insert cell above\n",
    "- ESC + B: Insert cell below\n",
    "- ESC + X: Delete cell\n",
    "- CTRL + ENTER: Execute cell\n",
    "- SHIFT + ENTER: Execute cell and **jump to the next cell**\n",
    "- ESC + H: HELP, see all such shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise1'>Exercise 1 - Data validation / completeness check</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read csv file\n",
    "    pandas.read_csv('folder_name/filename.csv'): Loads csv file into Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file and save in the variable coil-details\n",
    "\n",
    "data_raw = pd.read_csv('data/Process-data_Recipe-A.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we remove sep=';' and decimal=','?\n",
    "\n",
    "\n",
    "Let's take a look at a data. Here are the few ways to do that:\n",
    "   \n",
    "#### Inspect the file\n",
    "    .head(n): Review first n rows of dataset\n",
    "    .tail(n) : Review last n rows of dataset\n",
    "    .columns : Review columns names of the dataset\n",
    "    .shape : Returns size (number of rows and number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a column in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw['glass_temp_zone1']  # Pass the column name as parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.glass_temp_zone1  # Every column with a label can be named directly with .<col_name>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data checking is made simpler by being able to inspect the data visually\n",
    "Pandas (and these table-like objects) includes simple plotting functions.\n",
    "\n",
    "    data['variable_name'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw['glass_temp_zone3'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a histogram of a column, we can use\n",
    "\n",
    "    data['variable_name'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw['glass_temp_zone3'].plot(kind=?????)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the Descriptives and look for anomalies\n",
    "    describe(): Creates a summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of descriptives for every column\n",
    "data_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of descriptives for the column 'glass_temp_zone3'\n",
    "data_raw[?????].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to look only at a subset of columns. We can select the columns we want by putting the in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.copy() #Copy the data_frame\n",
    "\n",
    "chosen_columns = ['time', 'glass_temp_zone1', 'glass_temp_zone3', 'pressing_pressure', 'RPM', 'cycle_time', 'glass_ID',\n",
    "                 'batch', 'recipe',]\n",
    "\n",
    "data = data[chosen_columns] #Select relevant columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then control the data more efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of descriptives for the new dataframe\n",
    "data.?????()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values and number of unique values\n",
    "\n",
    "For Categorie variables, like HotMill_Code, we want to know how many unique values there are, and how many of each value.  We can do this with :: \n",
    "\n",
    "    unique(): Get unique values\n",
    "    nunique(): Get the count of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see again what the columns are\n",
    "data.????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unique values of 'batch'?\n",
    "data['batch'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique values are there for 'batch'?\n",
    "data['batch'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique values are there for all columns?\n",
    "?????.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of missing values (NaNs)\n",
    "\n",
    "**NaN**s are cells without values. We can get number of empty cells per each column or each row by combining these two formulas:\n",
    "\n",
    "    isna(): Returns True (1) if value is NA (missing) and False (0) otherwise\n",
    "    sum(): Sums values\n",
    "        sum(axis = 1):  Sum values by rows\n",
    "        sum(axis = 0):  Sum values by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which cells are NA ?\n",
    "data_missing = data.isna()\n",
    "data_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sum them also by column\n",
    "data_missing.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(How do we count missing values per row?)\n",
    "\n",
    "Jupyter allows us to \"chain\" those operations in one line of code ( .isna() followed by .sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's count the number of missing values by column in one line of code\n",
    "data.????().????(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise2'>Exercise 2 - Data Aggregation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate (group) the data on one variable\n",
    "    groupby() Group data (similar to a pivot table in Excel)\n",
    "    mean(): Calculates the mean value\n",
    "    std(), median(), min(), max(): Calculate the standard deviation, the median, the minimum, the maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the 'glass_temp_zone3' column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['glass_temp_zone3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the mean of 'glass_temp_zone3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute the mean\n",
    "data['glass_temp_zone3'].?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate mean 'glass_temp_zone3' for each 'batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With groupby, we will get the mean for each 'batch'.\n",
    "\n",
    "# Create a new grouping object with grouping on 'batch'.   ->  coil_details.groupby('batch')\n",
    "# Then select the column you're interested in              -> ['glass_temp_zone3']\n",
    "# and get the mean.                                        -> .mean()\n",
    "\n",
    "data.groupby('batch')[?????].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation of multiple variables\n",
    "\n",
    "If we're interested in all variables grouped by 'batch', we don't have to specify one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby('batch').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some other aggregating functions like .std(), .max(), .min(), .median(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count unique values of 'cycle_time'\n",
    "    groupby(): Group data\n",
    "    nunique(): Count number of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now want to see number of unique 'cycle_time's used in each batch. \n",
    "\n",
    "To do so, we need to first group values by 'batch' and then count unique values of 'cycle_time'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just do see the columns we're interested in\n",
    "data[['batch', 'cycle_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cycle_time'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With groupby, we will get the number of different 'cycle_time' for each 'batch'.\n",
    "data.groupby(?????)['cycle_time'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise: grouped IQR\n",
    "\n",
    "#### Now lets try to get the inter-quartile range of  'glass_temp_zone3' for each 'batch'\n",
    "\n",
    "\n",
    "Let's define the inter-quartile range (IQR) first :\n",
    "\n",
    "![IQR](https://i2.wp.com/makemeanalyst.com/wp-content/uploads/2017/05/IQR-1.png?resize=431%2C460)\n",
    "\n",
    "So to compute the IQR we have to compute the first and the third quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With groupby, we will get the first quartile of  'glass_temp_zone3' for each 'batch'\n",
    "grouped_first_quantile = data.groupby('batch')['glass_temp_zone3'].quantile(0.25)\n",
    "grouped_first_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With groupby, we will get the third quartile of  'glass_temp_zone3' for each 'batch'\n",
    "grouped_third_quantile = data.groupby('batch')['glass_temp_zone3'].??????\n",
    "grouped_third_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the interquartile range (3rd quartile - 1st quartile)\n",
    "grouped_inter_quantile_range = grouped_third_quantile - ????????\n",
    "grouped_inter_quantile_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: does .describe() work also for grouped data? Of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.groupby('batch')['glass_temp_zone3'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise3'>Exercise 3 - Perform format corrections</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when we import data the type of the variable is not correct.\n",
    "\n",
    "    dtypes : Check the type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'time' is given as an \"object\" (usually means text-string), but we know it's a timestamp!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change time columns\n",
    "    pd.to_datetime(): Convert argument to datetime\n",
    "    pd.Timedelta() : Add/Subtract change to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 'Time' is in text-format, which python does not consider as a date/time format. Let's convert it to **datetime format**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(data['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, this just doesn't look right, the second position is a month, not a day. Input text data is DD/MM/YYYY format (otherwise it would be in the future!) \n",
    "\n",
    "Let's fix it by adding the argument dayfirst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(data['time'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs we just saw are not saved anywhere yet. We need to overwrite the original column with the new format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time'] = pd.??????(data['time'], dayfirst=True)\n",
    "data['time'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding/subtracting time to datetime column\n",
    "\n",
    "    pd.Timedelta(): amount of time that can be added/subtracted from datetime format column\n",
    "    \n",
    "    The input of which amount of time is flexible. Some examples:\n",
    "        pd.Timedelta('1H')\n",
    "        pd.Timedelta('01:00:00')\n",
    "        pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we notice that the timstamps are shifted by an hour compared to the real time.\n",
    "\n",
    "This can have many reasons (server in UTC time, daylight savings time, other time zone, etc.)\n",
    "\n",
    "We can make a new corrected timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['time_corrected'] = data['time'] - pd.Timedelta(????)\n",
    "data[['time', 'time_corrected']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise: Simple line plot of time series\n",
    "\n",
    "When we plotted the data as a line before, we didn't have real time on the x-axis, because this plotting function automatically uses the **index**\n",
    "\n",
    "If we don't specify what the index is, this will jhsut be a count from 0 in the order the rows appear in the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['glass_temp_zone3'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set our formatted datetime column as the index, the plots will better describe the real time series\n",
    "\n",
    "    data.index = data['name_of_column']        (keeps original column)\n",
    "    data = data.set_index('name_of_column')    (moves column to index only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = data['time_corrected']\n",
    "data['glass_temp_zone3'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise4'>Exercise 4 - Join and Merge datasets</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to associate dataframes. For example : \n",
    "\n",
    "    concat(): Concatenate together 2 or more tables\n",
    "\n",
    "![concat image](https://pandas.pydata.org/pandas-docs/stable/_images/merging_concat_basic.png)\n",
    "\n",
    "    merge(): Joins together 2 tables according to common columns\n",
    "\n",
    "    \"Inner\" merge: only keep rows with IDs in both tables\n",
    "![merge image](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key.png)\n",
    "\n",
    "    \"Outer\" merge: keep all rows, even those who cannot be matched up (here also with two keys!)\n",
    "![merge image](https://pandas.pydata.org/pandas-docs/stable/_images/merging_merge_on_key_outer.png)\n",
    "\n",
    "\n",
    "For more information go to : https://pandas.pydata.org/pandas-docs\n",
    "\n",
    "\n",
    "Our specific exercise will look a bit like this (but with more rows and columns!):\n",
    "\n",
    "![](images/structuring_exercise.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data\n",
    "    pandas.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A = pd.read_csv('data/Process-data_Recipe-A.csv', sep=';', decimal=',')\n",
    "data_B = pd.read_csv('data/Process-data_Recipe-B.csv', sep=';', decimal=',')\n",
    "data_C = pd.read_csv('data/Process-data_Recipe-C.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_C.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_data = [data_A, data_B, data_C]\n",
    "\n",
    "data_process_all = pd.concat(??????)\n",
    "\n",
    "# Correct the timestamp format like we did before\n",
    "data_process_all['time'] = pd.to_datetime(data_process_all[?????], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have quality data stored as Excel files in the same folder. We can read all of these the same way as with the process data, but with a new function for reading the file:\n",
    "\n",
    "    pd.read_excel('folder_name/filename.xlsx')\n",
    "    \n",
    "The function will read everything on an Excel sheet like one table, starting from the upper left corner.\n",
    "\n",
    "Make sure there is not any other data or text on that particular sheet that shouldn't go in the table.\n",
    "\n",
    "The default is to read the first sheet, but you can specify a name with the argument sheet_name='name_of_your_excel_sheet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A_q = pd.read_excel('data/Quality-data_Recipe-A.xlsx')\n",
    "data_B_q = pd.read_excel('data/Quality-data_Recipe-B.xlsx')\n",
    "data_C_q = pd.read_excel('data/Quality-data_Recipe-C.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_data = [????, ????, ????]\n",
    "\n",
    "data_quality_all = pd.concat(list_with_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge process and quality data on a unique identifier\n",
    "\n",
    "In both quality_data_all and production_data_all, we have the column 'glass_ID'. If we want to merge the two tables, we shouldn't simply concatenate them together, but we want to match some identifier available in both data sets.\n",
    "\n",
    "    df1.merge(df2, on='my_column', how='inner'): Merge together 2 tables df1 and df2 according to 'my_column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_merged = data_process_all.merge(data_quality_all, on=??????, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you check the shape of the data, i.e. the number of observations (rows) and features/variables (columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use how='outer' instead of 'inner'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily save this new combined table back to a csv or Excel file with:\n",
    "\n",
    "    data.to_csv('path/filename.csv')\n",
    "    data.to_excel('path/filename.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.to_csv('data/merged_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise: reading many files automatically\n",
    "\n",
    "If there are many files that need to be read and processed, there are different ways in Python to achieve this. Here is an example where **all** csv-files in the folder \"data\" called Process\\*.csv (* could be anything) are read in and concatenated the same way.\n",
    "\n",
    "#### Collect all files from a folder\n",
    "\n",
    "We have seen how to collect several files. Yet we had to provide the individual name of each file to read. If we have multpile files in a given folder there are solutions to:\n",
    "- detect all the files in the folder (i.e get all file names).\n",
    "- read each file based on the names collected in the previous step.\n",
    "- gather their content in a single dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of filepaths with the \"glob\" library\n",
    "# EXPLANATION HEREEREREERE and the *\n",
    "process_files = glob('data/Process*.csv')\n",
    "\n",
    "# Initializing an empty list that we can fill with the data tables\n",
    "list_with_data = []\n",
    "\n",
    "# Loop through all available filepaths and read like before\n",
    "for filepath in process_files:\n",
    "    file_data = pd.read_csv(filepath, sep=';', decimal=',')\n",
    "    \n",
    "    # Append (=add) the data from the current file to the list (that will grow longer every loop)\n",
    "    list_with_data.append(file_data)\n",
    "\n",
    "# Concatenate all data available in the list together\n",
    "data_process_all = pd.concat(list_with_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it doesn't matter if we have 3 or 100 different data files, all of them will be concatenated the same way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise5'>Exercise 5: Time Shifting</a>\n",
    "\n",
    "Let's say we have the hypothesis that the temperature in the process ('temp_chamber07') are correlated to the temperature outside (weather data). We can usually find relevant weather data somewhere online, in this case is is sampled every hour. \n",
    "\n",
    "First we take the column 'temp_chamber07' (but could also just take all data) from the csv-file we saved earlier with the merged data. If we now beforehand that we have a DateTime column, we can convert it to the correct format and set it as index directly in the read_csv() function:\n",
    "\n",
    "    index_col='time' (choose column with name)\n",
    "    parse_dates=True (choose to convert dates directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = pd.read_csv('data/merged_data.csv', index_col='time', parse_dates=True)\n",
    "\n",
    "data_chamber = data_merged[['temp_chamber07', 'temp_chamber05']]\n",
    "data_chamber.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we read temperature data from the relevant month from a csv file\n",
    "\n",
    "It consists in this case only of two columns, 'datetime' and 'local_temperature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58d3a12d20fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/local_temperature.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_temp = pd.read_csv('./data/local_temperature.csv', index_col=??????, parse_dates=True)\n",
    "data_temp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Shifting before joining\n",
    "\n",
    "We will be joining the two data sets on their timestamps, so by changing one of them, we can take delays between the data sets into account.\n",
    "\n",
    "#### Let's say that we suspect that there is a 30 min time delay between changes in outside weather and temperature and the effects on the process temperature. \n",
    "\n",
    "We can then shift the timestamp of the weather data by adding a pd.Timedelta() of 30 minutes (similarly to when we corrected the timestamp earlier). We already have the timestamp as index, so we have to change the index.\n",
    "\n",
    "        pd.Timedelta('1H')\n",
    "        pd.Timedelta('01:00:00')\n",
    "        pd.Timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp.index = data_temp.index + pd.Timedelta(?????)\n",
    "data_temp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise6'>Exercise 6: Equalizing time behaviour</a>\n",
    "\n",
    "Now that the absolute times should correspond to each other, we still have the problem that the data sets are sampled at different points in time. To compare these two, we have to equalize how they are sampled:\n",
    "\n",
    "- Let's say we're interested in a time-scale of 15 minutes\n",
    "- Our process data is sampled irregularly per glass unit (~every 20-30 seconds) => needs downsampling\n",
    "- The weather data is sampled every hour => needs upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "The process data needs to be downsampled to every 15 min (instead of every 20-30 s) \n",
    "    \n",
    "    resample() : set a new sampling frequency\n",
    "    mean(): calculate average value of a group (in this case, all samples in a 15 min interval)\n",
    "    max(): take the maximum value of a group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chamber_15min = data_chamber.resample('15min').mean()\n",
    "data_chamber_15min.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "The temperature data needs to be upsampled to every 15 min (instead of every every hour) \n",
    "    \n",
    "    resample() : set a new sampling frequency\n",
    "    mean(): calculate average value of a group\n",
    "    ffill(): fill the missing values with forward fill\n",
    "    bfill(): fill the missing values with backward fill\n",
    "    interpolate(): interpolate missing values linearly\n",
    "\n",
    "What happens when we pick mean() here like before? Can this also make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp_15min = data_temp.?????('15min').interpolate()\n",
    "data_temp_15min.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the data together\n",
    "\n",
    "To join the two data sets on the index, we can use\n",
    "    \n",
    "    data.join(other_data): automatically matches two datasets by index\n",
    "    \n",
    "which works in a similar way as merge(), but acting on the index of each table.\n",
    "\n",
    "![merge image](https://pandas.pydata.org/pandas-docs/stable/_images/merging_join.png)\n",
    "\n",
    "Note: exactly this behaviour can also be achieved with \n",
    "    pd.merge(df_left, df_right, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp_merge = data_chamber_15min.join(data_temp_15min)\n",
    "\n",
    "# Plot with two y-axes to better show the signals together\n",
    "# Create a figure and an \"axis\"-object to plot many things on\n",
    "fig, ax = plt.subplots(figsize=(14,5))\n",
    "\n",
    "# Plot the first signal, ax=ax means that we plot on the axis we created (and named \"ax\")\n",
    "data_temp_merge[['temp_chamber07', 'temp_chamber05']].plot(style='o', markersize=2, ax=ax)\n",
    "\n",
    "# The secondary_y argument puts this next plot on another y-axis to the right\n",
    "data_temp_merge[['local_temperature']].plot(style='o', markersize=2, secondary_y=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the process data, there are still gaps in the data (=15 min intervals that didn't contain any data)\n",
    "\n",
    "These can be handled now, or kept like this to interpolate/delete/impute later on (Exercise 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise7'>Exercise 7 - Calculating synthetic variables</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: If there are a lot of columns, we can easily see them all if we force Python to print all of them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We read data and list the columns\n",
    "list(data_merged.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate an interaction between pressing_time and pressing_pressure\n",
    "For example, we might be interested in the product of the two signals (row-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the column pressure_time_product\n",
    "data_merged['pressure_time_product'] = data_merged['pressing_time'] * ??????['pressing_pressure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the descriptives for the new column to see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = ['pressure_time_product', 'pressing_time', 'pressing_pressure']\n",
    "data_merged[var_list].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model the fingerprint of temperature across zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect that the \"fingerprint\" of the temperature between zones is a good way to describe quality.\n",
    "\n",
    "Let's first select the relevant columns and plot this fingerprint for the first 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = ['glass_temp_zone1', 'glass_temp_zone2', 'glass_temp_zone3', 'glass_temp_zone4']\n",
    "data_temp = data_merged[var_list].copy()\n",
    "\n",
    "# Plotting this fingerprint every 200th rows to see the trend. (data.T transposes the table = flips on diagonal)\n",
    "data_temp.iloc[::200].T.plot(figsize=(10,6))\n",
    "plt.ylabel('Temperature [C]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate change from zones 1 to 3 and 3 to 4\n",
    "\n",
    "Let's say the fingerprint we want is the **increase/decrease in temperature in percent** from\n",
    "- zone1 to zone3\n",
    "- zone3 to zone4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp['change_1_3'] = (data_temp['glass_temp_zone3']-data_temp['glass_temp_zone1']) / data_temp['glass_temp_zone1'] * 100\n",
    "data_temp['change_3_4'] = (data_temp[????]-data_temp[????]) / data_temp['glass_temp_zone3'] * 100\n",
    "data_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Historic variables\n",
    "Let's say the process experts believe that the efficiency of a cycle is strongly influenced by the temperature of the previous cycles. For example, we can look at the temperature of the previous cycle, or the average of a number of previous cycles.\n",
    "   \n",
    "    shift(x) : shift the column by x values\n",
    "    rolling(x, center=False) : provide a window of x values\n",
    "    mean(): average value during a period\n",
    "\n",
    "What does the center=False do for rolling()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With .iloc[start_index_nr:stop_index_nr], we can extract only a slice of the data (counted from 0, the 200 first rows)\n",
    "data_short = data_temp.iloc[:200].copy()\n",
    "\n",
    "# Shift by one row\n",
    "data_short['glass_temp_zone3_shift'] = data_short['glass_temp_zone3'].shift(-1)\n",
    "\n",
    "# Calculate moving average over 10 rows\n",
    "data_short['glass_temp_zone3_rolling'] = data_short['glass_temp_zone3'].rolling(10, center=False).mean()\n",
    "\n",
    "# Plot the three columns\n",
    "data_short[['glass_temp_zone3', 'glass_temp_zone3_shift', 'glass_temp_zone3_rolling']].plot(figsize=(15,4))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise8'>Exercise 8 - Handling outliers</a>\n",
    "\n",
    "### Depending on the type of outliers (bad data, abnormal real data, etc.) handling of these can be done at different points during the process: in the beginning, during or at the end of cleaning/structuring. Bad data would typically be handled in the beginning before interpolation, clalculating synthetic variables, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find all the outliers in variable 'glass_temp_zone1' by visual inspection using a histogram    \n",
    "    data.plot(kind='hist'): Plot a histogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged[????].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['glass_temp_zone1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude outlier values and create new data frame\n",
    "\n",
    "We can take only a subset of the data by indexing with a list of True/False values.\n",
    "These lists are for example outputs from logical operations like\n",
    "\n",
    "    > greater than\n",
    "    < lesser than\n",
    "    == equal to\n",
    "    != not equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['glass_temp_zone1'] != 0 # Is the value NOT equal to 0 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this list output (don't have to save it in between) and put it in brackets after the table:\n",
    "\n",
    "    data_new = data[some_list_of_true_or_false]: filters new data to only include the rows with True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_merged_no_outliers = data_merged[data_merged['glass_temp_zone1'] ?? 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it works. Plot histogram of filtered values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_no_outliers['glass_temp_zone1'].plot(kind='hist', bins=50) #The bins represent the number of bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The process experts feel that removing the top and bottom 5 percentile is also a good approach. How would you implement that? How does this cleaning approach compare to having a threshold?\n",
    "\n",
    "    quantile(nth): Returns nth percentile of the empirical distribution\n",
    "\n",
    "We can chain multiple logical operations with the & (AND) or | (OR) operators:\n",
    "\n",
    "    (data['column'] > 0) & (data['column'] < 10): only returns values where 'columm' is above 0 AND below 10\n",
    "    (data['column'] < 0) & (data['column'] > 10): only returns values where 'columm' is below 0 OR above 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the calculated qunatiles to use as limits\n",
    "lower_lim = data_merged['glass_temp_zone1'].quantile(0.05)\n",
    "upper_lim = data_merged['glass_temp_zone1'].quantile(0.95)\n",
    "\n",
    "\n",
    "data_percentile = data_merged[(data_merged['glass_temp_zone1'] < ???? ) & (data_merged['glass_temp_zone1'] > ????)]\n",
    "data_percentile['glass_temp_zone1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the special case of picking an interval between two limits, it's shorter to use the function .between()\n",
    "\n",
    "    data['column'].between(0,10): returns a True/False list, True if the values are between the limits\n",
    "    \n",
    "The same operation as before is now nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_percentile = data_merged[data_merged['glass_temp_zone1'].????(lower_lim, upper_lim)]\n",
    "data_percentile['glass_temp_zone1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_percentile['glass_temp_zone1'].plot(kind='hist', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above method REMOVES THE WHOLE ROWS where 'glass_temp_zone1' is <1\n",
    "\n",
    "### If we suspect that it's for example a sensor error, it might make more sense to just replace these value with NaN and keep the rest of the row data. Then we can choose later what to do with the gaps (next exercise!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will set all elements in the column ehere it is equal to 0 to NaN\n",
    "data_merged.loc[(data_merged['glass_temp_zone1'] == 0), 'glass_temp_zone1' ] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace outliers outside of one or two thresholds with the threshold values\n",
    "\n",
    "    pd.Series.clip(lower=x, upper=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged_no_outliers['glass_temp_zone1_clipped'] = data_merged_no_outliers['glass_temp_zone1'].clip(lower=653, upper=656)\n",
    "\n",
    "data_merged_no_outliers[['glass_temp_zone1', 'glass_temp_zone1_clipped']].plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='exercise9'>Exercise 9 - Handling missing values</a>\n",
    "\n",
    "Missing values could be caused by different things:\n",
    "\n",
    "- Values missing in raw data\n",
    "- Bad data or outliers removed from data set\n",
    "- Gaps left after equalizing time behaviour and resampling\n",
    "\n",
    "\n",
    "#### Identify rows with missing values in variable Temperature_SCS_zone\n",
    "    isna(): Check for missing values (returns list of True/False)\n",
    "    dropna(): Remove missing values from the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.isnull().sum(axis=??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do with the missing values?\n",
    "\n",
    "- Remove the whole rows where the values in one or more columns are missing\n",
    "- Remove a whole column that has a lot of missing values\n",
    "- Imputate/interpolate to \"fake\" real values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing whole rows where data is missing\n",
    "\n",
    "    data.dropna(subset=list_of_columns, how=method): drops rows containing NaN and returns a new table\n",
    "    \n",
    "    subset: if we want to look for NaNs only in some columns. If nothing is given, all columns are considered\n",
    "    how:\n",
    "        'any': removes row where at least one column (of the subset) is NaN\n",
    "        'all': only removes rows where ALL columns are NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep only rows that are complete in all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped_rows = data_merged.dropna()\n",
    "data_dropped_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, this throws away a lot of data! \n",
    "#### For example, the column 'optics_ok' has 506 missing values, and thus at least 506 rows will be dropped.\n",
    "\n",
    "It might make more sense to make sure that the important columns that you care more about are complete. Let's only drop the rows where any of 'glass_temp_zone1' and 'glass_temp_zone2' are NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped_rows = data_merged.dropna(subset=['glass_temp_zone1', 'glass_temp_zone2'], how=????)\n",
    "data_dropped_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a whole column where data is missing\n",
    "\n",
    "#### Since 'optics_ok' and 'geometry_name' have such a large amount of missing values, it might make sense to drop them all together:\n",
    "\n",
    "    data.drop(list_of_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dropped_columns = data_merged.drop(['optics_ok', 'geometry_name'], axis=1)\n",
    "data_dropped_columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate or impute missing values\n",
    "\n",
    "If we want to keep as much data as possible, it might be a good idea to fill the missing values with a replacement (imputation) or an interpolation from the surrounding values (for example in chronological order).\n",
    "\n",
    "    interpolate():    linearly interpolates missing values with the surrounding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where in time where the missing values are by plotting them as the median\n",
    "\n",
    "var = 'glass_temp_zone1'\n",
    "data_merged[var].plot(style='o', markersize=2, figsize=(15,4))\n",
    "data_merged[var][data_merged[var].isnull()].fillna(value=data_merged[var].median()).plot(style='x', markersize=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged['glass_temp_zone1'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intp = data_merged.copy()\n",
    "\n",
    "data_intp['glass_temp_zone1'] = data_intp['glass_temp_zone1'].??????()\n",
    "\n",
    "data_intp['glass_temp_zone1'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "To replace the missing values with a fix value (like the median, although in this case it doesn't seem to make sense), or backfill/forwardfill, we can use the function \n",
    "\n",
    "    fillna(value=x): replace missing values with value x\n",
    "    or\n",
    "    fillna(method='ffill'): replace missing values with the previous value ('bfill' for following value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fill = data_merged.copy()\n",
    "\n",
    "data_fill['glass_temp_zone1'] = data_fill['glass_temp_zone1'].fillna(method=?????)\n",
    "\n",
    "data_fill['glass_temp_zone1'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you have now learned how to read, structure, analyze, clean and save your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "Extend pandas table view. Pandas has an options system that lets you customize some aspects of its behaviour, display-related options being those the user is most likely to adjust.\n",
    "\n",
    "To plot all columns of a DataFrame, you can uncomment the lines below to change the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
